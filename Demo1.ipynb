{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3819a54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:06.096219Z",
     "start_time": "2024-01-04T02:50:06.060012Z"
    }
   },
   "outputs": [],
   "source": [
    "from sqlparser import parse_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af84efe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:06.111224Z",
     "start_time": "2024-01-04T02:50:06.098220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'select_fields': ['max(budget_in_billions)', 'min(budget_in_billions)'], 'tables': ['department'], 'conditions': [], 'group_by': ['name'], 'order_by': ['amount'], 'having': []}\n"
     ]
    }
   ],
   "source": [
    "# 测试函数\n",
    "sql = \"SELECT max(budget_in_billions) ,  min(budget_in_billions) FROM department GROUP BY name ORDER BY amount\"\n",
    "print(parse_sql(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be442b14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:06.126732Z",
     "start_time": "2024-01-04T02:50:06.113235Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    for item in data:\n",
    "        # 将SQL字典转换为文本\n",
    "        sql_text = \" \".join([\" \".join(item['parsed_sql'][key]) for key in item['parsed_sql']])\n",
    "        inputs.append(sql_text)\n",
    "        \n",
    "        # 拼接所有的问题作为输出\n",
    "        questions = \" \".join([step['question'] for step in item['interactive_steps']])\n",
    "        outputs.append(questions)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "file_path = 'training_set.json'\n",
    "data = load_dataset(file_path)\n",
    "inputs, outputs = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9bbfccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:06.142319Z",
     "start_time": "2024-01-04T02:50:06.128732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 141\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(text.split())\n",
    "    return {word: i+1 for i, word in enumerate(counter)}  # 从1开始编号\n",
    "\n",
    "# 创建词汇表\n",
    "vocab = build_vocab(inputs + outputs)\n",
    "vocab_size = len(vocab) + 1  # 加1是因为0通常用于padding\n",
    "\n",
    "print(f\"Vocab Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1508a73a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:18.112575Z",
     "start_time": "2024-01-04T02:50:06.145392Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0864cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:18.127864Z",
     "start_time": "2024-01-04T02:50:18.114476Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# def loss_function(real, pred):\n",
    "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "#     loss_ = loss_object(real, pred)\n",
    "\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#     loss_ *= mask\n",
    "\n",
    "#     return tf.reduce_mean(loss_)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    print(\"Mask:\", mask.numpy())  # 打印掩码\n",
    "    print(\"Unreduced loss:\", loss_.numpy())  # 打印未缩减的损失\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    reduced_loss = tf.reduce_mean(loss_)\n",
    "    print(\"Reduced loss:\", reduced_loss.numpy())  # 打印缩减后的损失\n",
    "    return reduced_loss\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f556d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:18.143579Z",
     "start_time": "2024-01-04T02:50:18.129872Z"
    }
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(inp, targ, enc_hidden):\n",
    "#     loss = 0\n",
    "\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "#         dec_hidden = enc_hidden\n",
    "\n",
    "#         dec_input = tf.expand_dims([vocab['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "#         for t in range(1, targ.shape[1]):\n",
    "#             predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "#             loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "#             dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "#     batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "#     variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "#     gradients = tape.gradient(loss, variables)\n",
    "\n",
    "#     optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "#     return batch_loss\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['start']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            # 打印预测和目标值的示例\n",
    "            if t == 1:\n",
    "                print(\"Sample predictions:\", predictions[0])\n",
    "                print(\"Sample target:\", targ[0])\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    print(\"Step loss:\", batch_loss.numpy())  # 打印每步的损失\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe64b6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:18.159468Z",
     "start_time": "2024-01-04T02:50:18.145602Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# 清除原有的特殊标记（如果有的话）\n",
    "inputs = [text.replace('<start> ', '').replace(' <end>', '') for text in inputs]\n",
    "outputs = [text.replace('<start> ', '').replace(' <end>', '') for text in outputs]\n",
    "\n",
    "# 重新添加特殊标记\n",
    "inputs = ['start ' + text + ' end' for text in inputs]\n",
    "outputs = ['start ' + text + ' end' for text in outputs]\n",
    "\n",
    "# 创建新的Tokenizer实例\n",
    "tokenizer = Tokenizer(oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(inputs + outputs)\n",
    "\n",
    "# 将文本转换为序列\n",
    "input_seqs = tokenizer.texts_to_sequences(inputs)\n",
    "output_seqs = tokenizer.texts_to_sequences(outputs)\n",
    "\n",
    "# 获取最长序列长度\n",
    "max_length_input = max(len(seq) for seq in input_seqs)\n",
    "max_length_output = max(len(seq) for seq in output_seqs)\n",
    "\n",
    "# 序列填充\n",
    "input_seqs = pad_sequences(input_seqs, maxlen=max_length_input, padding='post')\n",
    "output_seqs = pad_sequences(output_seqs, maxlen=max_length_output, padding='post')\n",
    "\n",
    "# 设置批处理大小\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea957d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:24.553079Z",
     "start_time": "2024-01-04T02:50:18.161444Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将数据转换为TensorFlow Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_seqs, output_seqs))\n",
    "dataset = dataset.shuffle(len(input_seqs)).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e65fceae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:24.801582Z",
     "start_time": "2024-01-04T02:50:24.555297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.0000\n",
      "Epoch 2 Loss 0.0000\n",
      "Epoch 3 Loss 0.0000\n",
      "Epoch 4 Loss 0.0000\n",
      "Epoch 5 Loss 0.0000\n",
      "Epoch 6 Loss 0.0000\n",
      "Epoch 7 Loss 0.0000\n",
      "Epoch 8 Loss 0.0000\n",
      "Epoch 9 Loss 0.0000\n",
      "Epoch 10 Loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 设置模型参数\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 由于使用了tokenizer，词汇表大小根据tokenizer的词汇表确定\n",
    "\n",
    "# 实例化编码器和解码器\n",
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size, embedding_dim, units)\n",
    "\n",
    "EPOCHS = 10\n",
    "steps_per_epoch = len(input_seqs) // BATCH_SIZE\n",
    "steps_per_epoch = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c8e55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:24.816854Z",
     "start_time": "2024-01-04T02:50:24.803465Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_parsed_sql(parsed_sql):\n",
    "    # 将parsed_sql转换为文本，并添加特殊标记\n",
    "    sql_text = \"start \" + \" \".join([\" \".join(parsed_sql[key]) for key in parsed_sql]) + \" end\"\n",
    "\n",
    "    # Tokenize输入\n",
    "    inputs = [tokenizer.word_index.get(word, tokenizer.word_index['<unk>']) for word in sql_text.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_input, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    # 模型预测\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['start']], 0)\n",
    "\n",
    "    for t in range(max_length_output):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += tokenizer.index_word.get(predicted_id, '') + ' '\n",
    "\n",
    "        if tokenizer.index_word.get(predicted_id) == 'end':\n",
    "            return result\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a42ebfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:50:31.109197Z",
     "start_time": "2024-01-04T02:50:24.818837Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: department records' include 'employee groups than and 'customer where department records' include 'employee groups than and 'customer where department records' include 'employee groups than and 'customer where department records' include 'employee groups than and 'customer where department records' include 'employee groups than and 'customer where department records' include 'employee groups than and 'customer \n"
     ]
    }
   ],
   "source": [
    "sample_parsed_sql = {\n",
    "    \"parsed_sql\": {\n",
    "        \"select_fields\": [\"name\", \"email\"],\n",
    "        \"tables\": [\"employees\"],\n",
    "        \"conditions\": [\"department = 'Sales'\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "prediction = evaluate_parsed_sql(sample_parsed_sql['parsed_sql'])\n",
    "print('Predicted translation:', prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
